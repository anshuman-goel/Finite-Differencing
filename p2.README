#############################################################################
#
# Group Info:
# agoel5 Anshuman Goel
# kgondha Kaustubh Gondhalekar
# ndas Neha Das
#
##############################################################################

Problem 3 (Group Problem)

Problem Objective: Given some function, numerically compute derivatives at all grid points, find all local minima and maxima, and the error and standard deviation of the error of these numerical computations. This has to be implemented in a parallel scheme using MPI which writes the results to an output file.

Solution Outline:
1. First, we divide the total grid points among the number of processors such that the last processor will have extra grid points corresponding to the remainder. We also calculate the chunk end points.
2. The above chunk size is used to construct the x array for the local grid points at each processor. Similarly, the y array is calculated at each processor which holds the fn(x) values.
3. Now, the boundary fn(x) values for each chunk are sent and received using MPI_Send and MPI_Recv for blocking communication, and MPI_Isend and MPI_Irecv for non-blocking communication. Boundary values for left-end of first processor and right-end of last processor are not sent or received.
4. Once that is complete, we calculate the approximate derivative, the local minima and maxima and the error at each processor.
5. The error array from each processor is sent to the root process which receives and combines all the error arrays including its own to form the global error array. This is used to calculate the average error and standard deviation at the root.
6. For calculating the local minima and maxima with Manual Reduction, the local_min_max array at each processor is sent to the root process which receives and checks all the local_min_max arrays including its own to form the global_min_max array.
7. For calculating the local minima and maxima with MPI_Reduce, we have a written a custom reduction operation called myOp which will append all the valid values in local_min_max arrays of all processors into a global_min_max array at root.

Commands to install/execute: Login to the ARC cluster and then login interactively to a compute node with the following commands -
srun -N1 -n4 -p opteron --pty /bin/bash
make -f p2.Makefile
prun ./p2_mpi

Performance Comparison:

We tested our results with grid points size: 100000, epsilon value: 0.00005 and degree: 3 for finite difference, error, and local minima maxima calculation across the four communication methods of blocking, non-blocking, manual reduction, and MPI_Reduce. (All results are in seconds)

Serial - Finite difference: 6.7e-03, error: 9.79e-03, local minima maxima: 4.02e-03

What the values mean:
	1: Blocking communication, 0: Non-Blocking communication
	1: Manual Reduction; 0 : MPI_REDUCE

BLOCKING 1
MANUALREDUCE 1

1 Node 4 processors

Total send-call Time(s) 6.189346e-04
Total Finite Derivative Time(s) 4.262924e-04
Total Error Time(s) 5.341291e-03
Total local_min_max Derivative Time(s) 6.198883e-06


2 Node 16 processors

Total send-call Time(s) 6.890297e-05
Total Finite Derivative Time(s) 1.034737e-04
Total Error Time(s) 4.287720e-02
Total local_min_max Derivative Time(s) 1.716614e-05


BLOCKING 0
MANUALREDUCE 1

1 Node 4 processors

Total send-call Time(s) 6.628036e-05
Total Finite Derivative Time(s) 4.134178e-04
Total Error Time(s) 4.780293e-03
Total local_min_max Derivative Time(s) 8.344650e-06

2 Node 16 processors

Total send-call Time(s) 6.484985e-05
Total Finite Derivative Time(s) 1.037121e-04
Total Error Time(s) 1.069188e-02
Total local_min_max Derivative Time(s) 1.502037e-05




BLOCKING 1
MANUALREDUCE 0

1 Node 4 processors

Total send-call Time(s) 6.628036e-05
Total Finite Derivative Time(s) 5.290508e-04
Total Error Time(s) 2.093554e-03
Total local_min_max Derivative Time(s) 1.131606e-02


2 Node 16 processors

Total send-call Time(s) 5.650520e-05
Total Finite Derivative Time(s) 8.058548e-05
Total Error Time(s) 2.733254e-02
Total local_min_max Derivative Time(s) 7.820129e-05



BLOCKING 0
MANUALREDUCE 0

1 Node 4 processors

Total send-call Time(s) 7.891655e-05
Total Finite Derivative Time(s) 6.246567e-04
Total Error Time(s) 6.143570e-03
Total local_min_max Derivative Time(s) 6.747246e-05

2 Node 16 processors

Total send-call Time(s) 8.511543e-05
Total Finite Derivative Time(s) 1.428127e-04
Total Error Time(s) 3.106785e-02
Total local_min_max Derivative Time(s) 8.630753e-05


As the results show, the calculation time of the parallel programs are lesser as compared to the serial program.
With the increase in the number of processors, the calculation time for finite derivative decreases while that of error and local minima maxima calculation increases and hence overall calculation time increases as communications between processors also increases.
An interesting observation is that while the blocking/non-blocking does not affect the manual reduction/MPI_Reduce methods, the manual reduction method for local minima maxima calculation takes lesser time as compared to the MPI_Reduce method.



Comment on how the errors change for (i) the number of grid points and (ii) the value of epsilon used.


(i)
The error is inversely proportional to the number of grid points. As the number of grid points increase, the error decreases since 'dx' approaches closer to zero.

(ii)
Change in epsilon value does not affect the error. Though it does affect the local minima and maxima detected.

Additionally, epsilon is inversely proportional to the number of grid points. This is also the expected behaviour since as the interval between two grid points decreases, the epsilon should also decrease. This is because, if  the epsilon is too large with respect to the number of grid points, we may detect multiple local minima/maxima that aren't correct (false positive).
